percent=rep(NA,length(covars))
for (i in 1:length(covars)){
temp=covars[i]
percent[i]=sum(is.na(all_info2[which(all_info2$id %in% target_people$id),temp]))/nrow(all_info2)
}
names(percent)=covars
for (i in 1:length(covars)){
#ith covariates
temp=covars[i]
print(temp)
#rows with missing values
missing = is.na(all_info2[,temp])
#averages
avg = mean(all_info2[,temp],na.rm = TRUE)
if (percent[i]<=0.1){
#if missing pattern is not severe, replace missing values with averages
all_info2[missing ,temp] = avg
}else{
#if missing pattern is severe, missing indicator method
cov_name = temp
print(cov_name)
new_col = paste0(cov_name,'_missing')
all_info2[,new_col] = missing * avg
all_info2[missing,temp] = 0
}
}
percent
save(net,all_info2,id_list,file=data_save_path)
rm(list=ls())
library(dplyr)
data_save_path='/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/'
data_save_path=paste0(data_save_path,'net_complete_natvillage.RData')
load(data_save_path)
subnet=get.inducedSubgraph(net,v=which(net %v%'vertex.names'%in% id_list))
o=as.sociomatrix(subnet)
o=diag(apply(o,1,sum))-o
#imputed outcomes
Y_sim2=matrix(0,length(id_list),12)
U=rlogis(length(id_list), location = 0, scale = 1)
#U = o %*% o %*% rnorm(length(id_list)) #preference shock
covar=all_info2[which(all_info2$id %in% id_list),c('id','male','age','agpop','ricearea_2010','literacy','risk_averse','disaster_prob')]
covar=as.matrix(covar)
beta=c(0.0216,0.0044,-0.0089,0.00436,0.0891,0.1091,0.0018) * 4 #from Table 2 column1 in the paper
Utility_index= covar[,2:8] %*% beta
Utility_index
#choose the intercept such that the base line take-up rate is 35 percent on average
intercept_seq=seq(0,5,by=0.02)
base_line= c()
for (intercept in intercept_seq){
base_line=c(base_line,mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept))))
}
#the intercept equals to 2.26
final=intercept_seq[which.min(abs(base_line-0.35))]
Utility_index = Utility_index - final
Utility_index= cbind(covar[,1],Utility_index)
intercept
final
intercept=0
+0.141/(0.35*0.65)
mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept)))
intercept
Utility_index= covar[,2:8] %*% beta
mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept)))
intercept_seq=seq(0,5,by=0.02)
base_line= c()
for (intercept in intercept_seq){
base_line=c(base_line,mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept))))
}
#the intercept equals to 2.26
final=intercept_seq[which.min(abs(base_line-0.35))]
Utility_index = Utility_index - final
Utility_index= cbind(covar[,1],Utility_index)
final
#2nd Exposure Mapping: First Round, Intensive Info (FR-I)
Y_sim2[,2]= (Utility_index[,2]+0.141/(0.35*0.65)) > U #0.141 from Table 2 Column 1
mean(Y_sim2[,2])
mfinal
final
#the intercept equals to 2.26
final=intercept_seq[which.min(abs(base_line-0.35))]
Utility_index = Utility_index - final
Utility_index= cbind(covar[,1],Utility_index)
#1st Exposure Mapping: First Round, Simple Info (FR-S)
Y_sim2[,1]= Utility_index[,2] > U
mean(Y_sim2[,1])
beta=c(0.0216,0.0044,-0.0089,0.00436,0.0891,0.1091,0.0018) * 4 #from Table 2 column1 in the paper
Utility_index= covar[,2:8] %*% beta
#choose the intercept such that the base line take-up rate is 35 percent on average
intercept_seq=seq(0,5,by=0.02)
base_line= c()
for (intercept in intercept_seq){
base_line=c(base_line,mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept))))
}
#the intercept equals to 2.26
final=intercept_seq[which.min(abs(base_line-0.35))]
Utility_index = Utility_index - final
Utility_index= cbind(covar[,1],Utility_index)
Utility_index
final
Utility_index[1,]
Utility_index= covar[,2:8] %*% beta
Utility_index[,1]
#choose the intercept such that the base line take-up rate is 35 percent on average
intercept_seq=seq(0,5,by=0.02)
base_line= c()
for (intercept in intercept_seq){
base_line=c(base_line,mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept))))
}
#the intercept equals to 2.26
final=intercept_seq[which.min(abs(base_line-0.35))]
Utility_index = Utility_index - final
Utility_index= cbind(covar[,1],Utility_index)
final
help(optim)
rm(list=ls())
#load files
data_save_path='/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/'
data_save_path=paste0(data_save_path,'net_complete_natvillage.RData')
load(data_save_path)
target_people=read.csv('/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/target_info.csv')
#covar data, dealing with missing values
covars = c('id','male','age', 'agpop',
'ricearea_2010','risk_averse', 'disaster_prob', 'literacy', 'takeup_survey')
#check missing patterns
percent=rep(NA,length(covars))
for (i in 1:length(covars)){
temp=covars[i]
percent[i]=sum(is.na(all_info2[which(all_info2$id %in% target_people$id),temp]))/nrow(all_info2)
}
names(percent)=covars
for (i in 1:length(covars)){
#ith covariates
temp=covars[i]
print(temp)
#rows with missing values
missing = is.na(all_info2[,temp])
#averages
avg = mean(all_info2[,temp],na.rm = TRUE)
if (percent[i]<=0.1){
#if missing pattern is not severe, replace missing values with averages
all_info2[missing ,temp] = avg
}else{
#if missing pattern is severe, missing indicator method
cov_name = temp
print(cov_name)
new_col = paste0(cov_name,'_missing')
all_info2[,new_col] = missing * avg
all_info2[missing,temp] = 0
}
}
percent
rm(list=ls())
library(statnet)
library(RColorBrewer)
library(ggplot2)
library(dplyr)
setwd("/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/")
#setwd("/Volumes/GoogleDrive/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/")
#load data
network_info=read.csv("network_info.csv")
length(unique(network_info$id))
all_info=read.csv("all_info.csv")
#create a net object
relation_data <- network_info[,c('id','network_id')]
net <- network(relation_data,matrix.type="edgelist")
#administrative villages
id_list=net %v% "vertex.names"
villages= all_info[match(id_list,all_info$id),"village"]
sum(is.na(villages))
net %v% "village" = villages
#natural village
id_list=net %v% "vertex.names"
nat_villages= all_info[match(id_list,all_info$id),"address"]
sum(is.na(nat_villages))
net %v% "nat_village" = nat_villages
#47 villages
length(unique(net%v%"village"))
#number of households
network.size(net)
#max in and out degree degree
max(degree(net,cmode="outdegree"))
max(degree(net,cmode="indegree"))
max(degree(net,cmode="freeman"))
#inspect graph
components(net,'weak')
#number of households
network.size(net)
#max degree
max(degree(net,gmode="graph"))
#plotting the graph
par(mfrow=c(1,1))
#for coloring
village_color = as.factor(get.vertex.attribute(net,'village'))
coul <- brewer.pal(11,'Spectral')
coul <- colorRampPalette(coul)(length(levels(village_color)))
legend_village = as.numeric(as.character(factor(village_color)))
temp=cbind(coul[village_color],legend_village )
temp=data.frame(temp)
color_list=distinct(temp)
color_list=color_list[order(as.numeric(color_list[,2])),]
#ploting
pdf(file="Friendships_Complete_Village_Info.pdf",
width = 8, height = 7, # Width and height in inches
bg = "white",          # Background color
colormodel = "cmyk",    # Color model (cmyk is required for most publications)
paper = "A4")
gplot(net,gmode='graph',mode="kamadakawai",vertex.col = coul[village_color], vertex.cex = 1.5,main="Reported Friendships, Complete Village Info")
legend("bottomleft",legend=color_list[order(as.numeric(color_list[,2]))[25:47],2],col=color_list[order(as.numeric(color_list[,2]))[25:47],1],pch=19,pt.cex=1.5,bty="n")
legend("bottomright",legend=color_list[order(as.numeric(color_list[,2]))[1:24],2],col=color_list[order(as.numeric(color_list[,2]))[1:24],1],pch=19,pt.cex=1.5,bty="n",
title="Admin Villages")
dev.off()
##############################################################################
########################component statistics##################################
##############################################################################
#plotting a component
membership=component.dist(net,connected='weak')$membership
membership_info= cbind(1:length(membership),membership)#info for graphing
net %v% "membership" = membership
compare=cbind(get.vertex.attribute(net,"membership"),get.vertex.attribute(net,"nat_village"))
colnames(compare)=c('membership','nat_village')
##check village belongings
nat_villages= unique(compare[,2])
result=matrix(0,length(nat_villages),2)
result[,1]=nat_villages
for (i in 1:length(nat_villages)){
temp_village=nat_villages[i]
membership_temp=compare[which(compare[,2]==temp_village),1]
result[i,2]=length(unique(membership_temp))
}
head(result[order(result[,2],decreasing=TRUE),])
#all_info2[all_info2$address==34,]
##check membership sizes
nat_cluster= unique(compare[,1])
result_c=matrix(0,length(nat_cluster),2)
result_c[,1]=nat_cluster
for (i in 1:length(nat_cluster)){
temp_cluster=nat_cluster[i]
membership_temp=compare[which(compare[,1]==temp_cluster),2]
#result_c[i,2]=length(unique(membership_temp))
result_c[i,2]=length((membership_temp))
}
##############################################################################
########################plot2#################################################
##############################################################################
##############################################################################
#No.7 component has the largest size 1914
#No.8 component has the second largest size 577
table(membership)
####Visulize the 8th component
pdf("directed_graphs_per_component.pdf" ,
width = 8, height = 7, # Width and height in inches
bg = "white",          # Background color
colormodel = "cmyk",    # Color model (cmyk is required for most publications)
paper = "A4")
progress=paste0('ploting ',7, 'th component....')
print(progress)
#extract ith subgraph
subgraph_info = membership_info[which(membership_info[,2]==8),]
subgraph_vertex = as.numeric(subgraph_info[,1])
subgraph=get.inducedSubgraph(net, subgraph_vertex, alters = NULL, eid = NULL)
#for coloring
village_color = as.factor(get.vertex.attribute(subgraph,'nat_village'))
coul <- brewer.pal(11,'Spectral')
coul <- colorRampPalette(coul)(length(levels(village_color)))
coul = sample(coul,length(coul)) #break color orders randomly
legend_village = as.numeric(as.character(factor(village_color)))
temp=cbind(coul[village_color],legend_village )
temp=data.frame(temp)
color_list=distinct(temp)
color_list=color_list[order(as.numeric(color_list[,2])),]
title=paste0("Reported Friendships",", Second Largest Component")
gplot(subgraph,gmode='graph',mode="kamadakawai", vertex.col=coul[village_color],
vertex.cex = 1.5,main=title)
legend("bottomleft",legend=color_list[,2],col=color_list[,1],pch=19,pt.cex=1.5,bty="n",title='Natural Village')
dev.off()
####Check correct match village assignment
index=sample(1:4000,20)
names = net %v% "vertex.names"
names = names[index]
village = net %v% "village"
village = village[index]
expect=cbind(names,village)
correct=all_info[which(all_info$id %in% names),c('id',"village")]
cbind(correct[match(expect[,'names'],correct$id),],expect)
#######################Code randomization group##########################
#natural village
nat_village=unique(all_info$address)
all_info['group']=9999
group_index=0
for (i in 1:length(nat_village)){
print(i)
#nat_village_level_randomization
nat_village_temp=nat_village[i]
all_info[which(all_info$address==nat_village_temp),'group']=group_index
group_index=group_index+1
}
#############################################################################
compare=cbind(get.vertex.attribute(net,"membership"),get.vertex.attribute(net,"vertex.names"))
colnames(compare)=c('membership','id')
all_info2=merge(all_info,compare,by='id')
#merge some clusters
all_info2[which(all_info2$membership==2),'membership']=1
all_info2[which(all_info2$membership==3),'membership']=1
all_info2[which(all_info2$membership==4),'membership']=1
all_info2[which(all_info2$membership==9),'membership']=6
all_info2[which(all_info2$membership==6),'membership']=12
all_info2[which(all_info2$membership==14),'membership']=15
all_info2[which(all_info2$membership==15),'membership']=13
all_info2[which(all_info2$membership==22),'membership']=24
all_info2[which(all_info2$membership==28),'membership']=26
all_info2[which(all_info2$membership==29),'membership']=26
all_info2[which(all_info2$membership==32),'membership']=31
all_info2[which(all_info2$membership==33),'membership']=35
all_info2[which(all_info2$membership==34),'membership']=35
all_info2[which(all_info2$membership==39),'membership']=41
#check membership and group
compare=cbind(all_info2[,'membership'],all_info2[,'group'])
nat_villages= unique(compare[,2])
result=matrix(0,length(nat_villages),3)
result[,1]=nat_villages
for (i in 1:length(nat_villages)){
temp_village=nat_villages[i]
membership_temp=compare[which(compare[,2]==temp_village),1]
result[i,2]=length(membership_temp)
result[i,3]=unique(membership_temp)
}
colnames(result) = c('nat_village','size','membership')
head(result[order(result[,2],decreasing=FALSE),])
#merge
result[result[,3]==8,]
result[result[,3]==11,]
#merge 79 80
all_info2[all_info2$group==80,'group']=79
#merge 47 and 48
all_info2[all_info2$group==47,'group']=48
#merge villages
group_index=unique(all_info2$group)
for (i in group_index){
ind=all_info2$group==i
nobs=sum(ind)
if (nobs<10){
print(nobs)
}
integer_part= nobs %/% 10
pattern=c(4,3,2,1,4,3,4,3,4,3)
temp=rep(pattern,integer_part+1)
temp=temp[1:nobs]
all_info2[ind,'assignment']=temp #order does not matter
}
id_list=net %v% "vertex.names"
group= all_info2[match(id_list,all_info2$id),"group"]
assignment=all_info2[match(id_list,all_info2$id),"assignment"]
membership=all_info2[match(id_list,all_info2$id),"membership"]
sum(is.na(villages))
net %v% "group" = group
net %v% "assignment"=assignment
net %v% "membership"= membership
write.csv(all_info2,'/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/all_info2_natvillage.csv')
data_save_path='/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/'
data_save_path=paste0(data_save_path,'net_complete_natvillage.RData')
id_list=unique(network_info$id)
save(net,all_info2,id_list,file=data_save_path)
#number of people in each villages
num_people=table(all_info2[,'address'])
num_people=cbind(as.numeric(rownames(num_people)),as.numeric(num_people))
colnames(num_people)=c('nat_village','num')
###Check two hop villages
nat_village_connect=merge(relation_data,all_info2[,c('id','address')],by=c('id'))
sum(is.na(nat_village_connect$address))
colnames(nat_village_connect)=c('id','network_id','ego_village')
nat_village_connect=merge(nat_village_connect,all_info2[,c('id','address')],by.x=c('network_id'),by.y=c('id'))
colnames(nat_village_connect)=c('id','network_id','ego_village','friend_village')
one_hop = distinct(nat_village_connect,nat_village_connect$ego_village,nat_village_connect$friend_village)
colnames(one_hop)=c('point_from','point_to')
two_hop = merge(one_hop,one_hop, by.x=c('point_to'),by.y=c('point_from'))
two_hop=two_hop[,c(1,3)]
colnames(two_hop)=c('point_from','point_to_two_hop')
two_hop_number_people=merge(two_hop,num_people,by.x=c('point_to_two_hop'), by.y=c('nat_village'))
two_hop_sums=as.data.frame(
two_hop_number_people %>% group_by(point_from) %>% summarise(sum(num)))
rm(list=ls())
library(dplyr)
library(statnet)
setwd('/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/Ranalysis/')
#import data, start with 23243 relations
data=read.csv('network.csv')
covar_raw = read.csv('covar.csv')
#covar data, dealing with missing values
covars = c('id','male','age', 'agpop',
'educ', 'ricearea_2010',
'rice_inc', 'disaster_yes', 'disaster_loss' ,
'risk_averse', 'disaster_prob', 'literacy', 'understanding','takeup_survey')
covar=covar_raw[,covars]
#only 2425 complete cases
sum(complete.cases(covar))
percent=rep(NA,ncol(covar))
for (i in 1:ncol(covar)){
percent[i]=sum(is.na(covar[,i]))/nrow(covar)
}
percent #disater_loss missing around 50%
names(percent)=covars
#This script imputes missingness of the data
rm(list=ls())
#load files
data_save_path='/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/'
data_save_path=paste0(data_save_path,'net_complete_natvillage.RData')
load(data_save_path)
target_people=read.csv('/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/target_info.csv')
#covar data, dealing with missing values
covars = c('id','male','age', 'agpop',
'ricearea_2010','risk_averse', 'disaster_prob', 'literacy', 'takeup_survey')
#check missing patterns
percent=rep(NA,length(covars))
for (i in 1:length(covars)){
temp=covars[i]
percent[i]=sum(is.na(all_info2[which(all_info2$id %in% target_people$id),temp]))/nrow(all_info2)
}
names(percent)=covars
percent
percetn*100
percent*100
exp(3)/(1+exp(3))
exp(-2.6)/(1+exp(-2.6))
max(Utility_index)
#data imputation for simulations:
rm(list=ls())
library(dplyr)
data_save_path='/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/'
data_save_path=paste0(data_save_path,'net_complete_natvillage.RData')
load(data_save_path)
subnet=get.inducedSubgraph(net,v=which(net %v%'vertex.names'%in% id_list))
o=as.sociomatrix(subnet)
o=diag(apply(o,1,sum))-o
#imputed outcomes
Y_sim2=matrix(0,length(id_list),12)
#base line take-up rate is 35 percent (P90; also can be computed from the dataset by
#using people from first group simple session)
#the paper used the following variables for adjustment (Table 2 column 1, coefficients are retrieved from the stata program)
# male age agpop ricearea_2010 literacy risk_averse disaster_prob
# the paper estimated a linear probability model, we convert the coefficients by using the rule
# beta_{LinearRegression}= 4beta_{LogisticRegression}. The 4 is the reciprocal of derivative of exp(x)/(1+exp(x))
# at x=0.
U=rlogis(length(id_list), location = 0, scale = 1)
#U = o %*% o %*% rnorm(length(id_list)) #preference shock
covar=all_info2[which(all_info2$id %in% id_list),c('id','male','age','agpop','ricearea_2010','literacy','risk_averse','disaster_prob')]
covar=as.matrix(covar)
beta=c(0.0216,0.0044,-0.0089,0.00436,0.0891,0.1091,0.0018) * 4 #from Table 2 column1 in the paper
Utility_index= covar[,2:8] %*% beta
max(Utility_index)
#data imputation for simulations:
rm(list=ls())
library(dplyr)
data_save_path='/Users/haoge/Library/CloudStorage/GoogleDrive-hgchang1508@gmail.com/My Drive/UnifiedDesignBasedInference/Cai(2015)Insurance/FinalData/'
data_save_path=paste0(data_save_path,'net_complete_natvillage.RData')
load(data_save_path)
subnet=get.inducedSubgraph(net,v=which(net %v%'vertex.names'%in% id_list))
o=as.sociomatrix(subnet)
o=diag(apply(o,1,sum))-o
#imputed outcomes
Y_sim2=matrix(0,length(id_list),12)
#base line take-up rate is 35 percent (P90; also can be computed from the dataset by
#using people from first group simple session)
#the paper used the following variables for adjustment (Table 2 column 1, coefficients are retrieved from the stata program)
# male age agpop ricearea_2010 literacy risk_averse disaster_prob
# the paper estimated a linear probability model, we convert the coefficients by using the rule
# beta_{LinearRegression}= 4beta_{LogisticRegression}. The 4 is the reciprocal of derivative of exp(x)/(1+exp(x))
# at x=0.
U=rlogis(length(id_list), location = 0, scale = 1)
#U = o %*% o %*% rnorm(length(id_list)) #preference shock
covar=all_info2[which(all_info2$id %in% id_list),c('id','male','age','agpop','ricearea_2010','literacy','risk_averse','disaster_prob')]
covar=as.matrix(covar)
beta=c(0.0216,0.0044,-0.0089,0.00436,0.0891,0.1091,0.0018) * 4 #from Table 2 column1 in the paper
Utility_index= covar[,2:8] %*% beta
Utility_index
max(Utility_index)
max(Utility_index)
max(Utility_index,na.rm=T)
mmin(Utility_index,na.rm=T)
min(Utility_index,na.rm=T)
exp(-3)/(1+exp(-3))
covar[,1]
covar[,2]
#U = o %*% o %*% rnorm(length(id_list)) #preference shock
covar=all_info2[which(all_info2$id %in% id_list),c('id','male','age','agpop','ricearea_2010','literacy','risk_averse','disaster_prob')]
#choose the intercept such that the base line take-up rate is 35 percent on average
intercept_seq=seq(0,5,by=0.02)
base_line= c()
for (intercept in intercept_seq){
base_line=c(base_line,mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept))))
}
#the intercept equals to 2.26
final=intercept_seq[which.min(abs(base_line-0.35))]
Utility_index = Utility_index - final
Utility_index= cbind(covar[,1],Utility_index)
final
U=rlogis(length(id_list), location = 0, scale = 1)
#U = o %*% o %*% rnorm(length(id_list)) #preference shock
covar=all_info2[which(all_info2$id %in% id_list),c('id','male','age','agpop','ricearea_2010','literacy','risk_averse','disaster_prob')]
covar[,5]
covar=as.matrix(covar)
beta=c(0.0216,0.0044,-0.0089,0.00436,0.0891,0.1091,0.0018) * 4 #from Table 2 column1 in the paper
Utility_index= covar[,2:8] %*% beta
#choose the intercept such that the base line take-up rate is 35 percent on average
intercept_seq=seq(0,5,by=0.02)
base_line= c()
for (intercept in intercept_seq){
base_line=c(base_line,mean(exp(Utility_index-intercept)/(1+exp(Utility_index-intercept))))
}
#the intercept equals to 2.26
final=intercept_seq[which.min(abs(base_line-0.35))]
Utility_index = Utility_index - final
Utility_index= cbind(covar[,1],Utility_index)
